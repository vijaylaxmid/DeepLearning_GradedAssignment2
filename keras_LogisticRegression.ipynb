{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_LogisticRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em0JjUJQ-QS9",
        "colab_type": "text"
      },
      "source": [
        "**Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQUXOIYh-RLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGLumXf9-XKC",
        "colab_type": "text"
      },
      "source": [
        "**Reading the dataset file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0qPACp07xlG",
        "colab_type": "code",
        "outputId": "ac238556-a0e7-4f66-c8ad-7d644d6ca72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "df = pd.read_csv('diabetes.csv')\n",
        "print(df.info())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfC9ll2Q-ci3",
        "colab_type": "text"
      },
      "source": [
        "**Plotting the counts for the 'Outcome' column (class labels)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJXyq3zu72OB",
        "colab_type": "code",
        "outputId": "0173b78c-64fc-445c-dc0d-caa0c5e438d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        " sb.countplot(x='Outcome', data=df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa67808f390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYKT8Hb8-gRw",
        "colab_type": "text"
      },
      "source": [
        "**Converting the dataframe into a numpy matrix.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG8SQ8fR8IDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_values = df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yme0NArU-kgb",
        "colab_type": "text"
      },
      "source": [
        "**Shuffling rows of the matrix.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W8iRVvk-oi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(df_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3_s712i8MhU",
        "colab_type": "code",
        "outputId": "352eb140-a6ff-430a-8eb0-9a03627ac30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Splitting the first N-1 columns as X.\n",
        "x = df_values[:,:-1]\n",
        "\n",
        "# Splitting the last column as Y.\n",
        "y = df_values[:, -1].reshape(x.shape[0], 1)\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Computing the class weights.\n",
        "# Note: This returns an ndarray.\n",
        "weights = class_weight.compute_class_weight('balanced', np.unique(y), y.ravel()).tolist()\n",
        "\n",
        "# Converting the ndarray to a dict.\n",
        "weights_dict = {\n",
        "    i: weights[i] for i in range(len(weights))\n",
        "}\n",
        "\n",
        "print(\"Class weights: \", weights_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 8)\n",
            "(768, 1)\n",
            "Class weights:  {0: 0.768, 1: 1.4328358208955223}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsEgafba8PJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Activation, Input\n",
        "import keras.regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeWhVKv-r9M",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate the model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRmH3I2t8TMW",
        "colab_type": "code",
        "outputId": "41cc9502-8635-4860-f94a-f1c0100dfbc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Add the input layer and the output layer.\n",
        "# The '1' indicates the number of output units.\n",
        "# The 'input_shape' is where we specify the dimensionality of our input instances.\n",
        "# The 'kernel_regularizer' specifies the strength of the L2 regularization.\n",
        "model.add(Dense(1, input_shape=(x.shape[1], ), kernel_regularizer=keras.regularizers.l2(0.017)))\n",
        "\n",
        "# Adding the BatchNorm layer.\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Adding the final activation, i.e., sigmoid.\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# Printing the model summary.\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_17 (Dense)             (None, 1)                 9         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1)                 4         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 13\n",
            "Trainable params: 11\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKh4UnuL8Wr3",
        "colab_type": "code",
        "outputId": "5f3c3890-a4c7-495d-c936-f6fd9de7ddcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Mean, columnar axis.\n",
        "x_mean = np.mean(x, axis=0, keepdims=True)\n",
        "\n",
        "# Std. Deviation, columnar axis.\n",
        "x_std = np.std(x, axis=0, keepdims=True)\n",
        "\n",
        "# Normalizing.\n",
        "x = (x - x_mean)/x_std\n",
        "\n",
        "print(x[:5, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.54791859 -0.68523633 -0.88431425 -0.3472913   0.12330164 -0.93826044\n",
            "   0.49869316 -1.04154944]\n",
            " [ 2.42174604  0.59794663  0.8217115   0.7818138  -0.69289057  0.6863059\n",
            "  -0.68821348  0.66020563]\n",
            " [-0.25095213  0.03459802  0.45982725 -1.28821221 -0.69289057 -1.14133123\n",
            "  -0.65801229  0.57511787]\n",
            " [ 0.04601433 -0.02799627 -0.05715025 -1.28821221 -0.69289057 -0.30366421\n",
            "   0.71614171  0.06459135]\n",
            " [-0.84488505  1.44296956  0.25303625 -0.22183517  0.55744644 -1.09056353\n",
            "  -0.07512938 -0.0204964 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsLHSNRj-xmI",
        "colab_type": "text"
      },
      "source": [
        "**Split into Train/test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4lTmEWi8Znc",
        "colab_type": "code",
        "outputId": "8e705fe0-cf2c-40bb-c7fb-cb879ea86ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the model into a 0.9-0.1 train-test split.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=5)\n",
        "\n",
        "print(\"Shape of x_train: \", x_train.shape)\n",
        "print(\"Shape of y_train: \", y_train.shape)\n",
        "print(\"Shape of x_test: \", x_test.shape)\n",
        "print(\"Shape of y_test: \", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train:  (691, 8)\n",
            "Shape of y_train:  (691, 1)\n",
            "Shape of x_test:  (77, 8)\n",
            "Shape of y_test:  (77, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXaQtV0-1lm",
        "colab_type": "text"
      },
      "source": [
        "**Compile the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d574YJmW8cNT",
        "colab_type": "code",
        "outputId": "1fe6bf98-fdaa-48ec-b574-66580d17e441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "print('Model compiled!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model compiled!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABZN9dQ48fc8",
        "colab_type": "code",
        "outputId": "80824625-68fc-44d4-989e-0f6f0a2183b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Initialize the Early Stopper.\n",
        "stopper = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
        "\n",
        "# Fit the data to the model and get the per-batch metric history.\n",
        "history = model.fit(x_train, y_train, validation_split=0.1, \n",
        "                    batch_size=128, epochs=700, \n",
        "                    callbacks=[stopper], class_weight=weights_dict, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 621 samples, validate on 70 samples\n",
            "Epoch 1/700\n",
            "621/621 [==============================] - 0s 370us/step - loss: 0.8055 - val_loss: 1.0059\n",
            "Epoch 2/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.8068 - val_loss: 0.9826\n",
            "Epoch 3/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7990 - val_loss: 0.9629\n",
            "Epoch 4/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7961 - val_loss: 0.9460\n",
            "Epoch 5/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7928 - val_loss: 0.9313\n",
            "Epoch 6/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.7916 - val_loss: 0.9184\n",
            "Epoch 7/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7865 - val_loss: 0.9069\n",
            "Epoch 8/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7780 - val_loss: 0.8969\n",
            "Epoch 9/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.7812 - val_loss: 0.8877\n",
            "Epoch 10/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.7767 - val_loss: 0.8792\n",
            "Epoch 11/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7689 - val_loss: 0.8713\n",
            "Epoch 12/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7688 - val_loss: 0.8642\n",
            "Epoch 13/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7688 - val_loss: 0.8577\n",
            "Epoch 14/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7641 - val_loss: 0.8517\n",
            "Epoch 15/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7619 - val_loss: 0.8460\n",
            "Epoch 16/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.7568 - val_loss: 0.8407\n",
            "Epoch 17/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.7549 - val_loss: 0.8357\n",
            "Epoch 18/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.7482 - val_loss: 0.8310\n",
            "Epoch 19/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.7492 - val_loss: 0.8265\n",
            "Epoch 20/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7423 - val_loss: 0.8224\n",
            "Epoch 21/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7391 - val_loss: 0.8185\n",
            "Epoch 22/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7398 - val_loss: 0.8147\n",
            "Epoch 23/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7348 - val_loss: 0.8110\n",
            "Epoch 24/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7324 - val_loss: 0.8076\n",
            "Epoch 25/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7303 - val_loss: 0.8043\n",
            "Epoch 26/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.7271 - val_loss: 0.8011\n",
            "Epoch 27/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.7237 - val_loss: 0.7981\n",
            "Epoch 28/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7223 - val_loss: 0.7951\n",
            "Epoch 29/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7192 - val_loss: 0.7923\n",
            "Epoch 30/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.7187 - val_loss: 0.7896\n",
            "Epoch 31/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7148 - val_loss: 0.7870\n",
            "Epoch 32/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7100 - val_loss: 0.7845\n",
            "Epoch 33/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.7114 - val_loss: 0.7822\n",
            "Epoch 34/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7085 - val_loss: 0.7798\n",
            "Epoch 35/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.7018 - val_loss: 0.7775\n",
            "Epoch 36/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.7062 - val_loss: 0.7752\n",
            "Epoch 37/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.7014 - val_loss: 0.7731\n",
            "Epoch 38/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6967 - val_loss: 0.7710\n",
            "Epoch 39/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6959 - val_loss: 0.7690\n",
            "Epoch 40/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6944 - val_loss: 0.7671\n",
            "Epoch 41/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6907 - val_loss: 0.7652\n",
            "Epoch 42/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6903 - val_loss: 0.7634\n",
            "Epoch 43/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.6864 - val_loss: 0.7616\n",
            "Epoch 44/700\n",
            "621/621 [==============================] - 0s 22us/step - loss: 0.6842 - val_loss: 0.7598\n",
            "Epoch 45/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.6855 - val_loss: 0.7581\n",
            "Epoch 46/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.6816 - val_loss: 0.7564\n",
            "Epoch 47/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6778 - val_loss: 0.7548\n",
            "Epoch 48/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6789 - val_loss: 0.7531\n",
            "Epoch 49/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6737 - val_loss: 0.7515\n",
            "Epoch 50/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6751 - val_loss: 0.7500\n",
            "Epoch 51/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6711 - val_loss: 0.7484\n",
            "Epoch 52/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6706 - val_loss: 0.7470\n",
            "Epoch 53/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6678 - val_loss: 0.7455\n",
            "Epoch 54/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6651 - val_loss: 0.7441\n",
            "Epoch 55/700\n",
            "621/621 [==============================] - 0s 15us/step - loss: 0.6633 - val_loss: 0.7427\n",
            "Epoch 56/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6657 - val_loss: 0.7413\n",
            "Epoch 57/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6608 - val_loss: 0.7400\n",
            "Epoch 58/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6564 - val_loss: 0.7387\n",
            "Epoch 59/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6558 - val_loss: 0.7374\n",
            "Epoch 60/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6568 - val_loss: 0.7361\n",
            "Epoch 61/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6559 - val_loss: 0.7349\n",
            "Epoch 62/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.6503 - val_loss: 0.7336\n",
            "Epoch 63/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.6471 - val_loss: 0.7324\n",
            "Epoch 64/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.6513 - val_loss: 0.7312\n",
            "Epoch 65/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6476 - val_loss: 0.7301\n",
            "Epoch 66/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6454 - val_loss: 0.7289\n",
            "Epoch 67/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6445 - val_loss: 0.7278\n",
            "Epoch 68/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6424 - val_loss: 0.7267\n",
            "Epoch 69/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6402 - val_loss: 0.7256\n",
            "Epoch 70/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6377 - val_loss: 0.7245\n",
            "Epoch 71/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.6389 - val_loss: 0.7235\n",
            "Epoch 72/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6353 - val_loss: 0.7224\n",
            "Epoch 73/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.6334 - val_loss: 0.7214\n",
            "Epoch 74/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6307 - val_loss: 0.7204\n",
            "Epoch 75/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6311 - val_loss: 0.7194\n",
            "Epoch 76/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6297 - val_loss: 0.7185\n",
            "Epoch 77/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6269 - val_loss: 0.7175\n",
            "Epoch 78/700\n",
            "621/621 [==============================] - 0s 24us/step - loss: 0.6268 - val_loss: 0.7165\n",
            "Epoch 79/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6259 - val_loss: 0.7156\n",
            "Epoch 80/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6220 - val_loss: 0.7147\n",
            "Epoch 81/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.6218 - val_loss: 0.7138\n",
            "Epoch 82/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6186 - val_loss: 0.7129\n",
            "Epoch 83/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6203 - val_loss: 0.7120\n",
            "Epoch 84/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6182 - val_loss: 0.7111\n",
            "Epoch 85/700\n",
            "621/621 [==============================] - 0s 23us/step - loss: 0.6169 - val_loss: 0.7103\n",
            "Epoch 86/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6135 - val_loss: 0.7095\n",
            "Epoch 87/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6149 - val_loss: 0.7086\n",
            "Epoch 88/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6128 - val_loss: 0.7078\n",
            "Epoch 89/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6154 - val_loss: 0.7070\n",
            "Epoch 90/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6094 - val_loss: 0.7061\n",
            "Epoch 91/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6119 - val_loss: 0.7053\n",
            "Epoch 92/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6077 - val_loss: 0.7045\n",
            "Epoch 93/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6064 - val_loss: 0.7037\n",
            "Epoch 94/700\n",
            "621/621 [==============================] - 0s 26us/step - loss: 0.6059 - val_loss: 0.7030\n",
            "Epoch 95/700\n",
            "621/621 [==============================] - 0s 22us/step - loss: 0.6032 - val_loss: 0.7022\n",
            "Epoch 96/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.6039 - val_loss: 0.7015\n",
            "Epoch 97/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6038 - val_loss: 0.7008\n",
            "Epoch 98/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.6020 - val_loss: 0.7000\n",
            "Epoch 99/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5975 - val_loss: 0.6993\n",
            "Epoch 100/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.6002 - val_loss: 0.6986\n",
            "Epoch 101/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5965 - val_loss: 0.6979\n",
            "Epoch 102/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5976 - val_loss: 0.6972\n",
            "Epoch 103/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5962 - val_loss: 0.6965\n",
            "Epoch 104/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5937 - val_loss: 0.6958\n",
            "Epoch 105/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5908 - val_loss: 0.6950\n",
            "Epoch 106/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5918 - val_loss: 0.6943\n",
            "Epoch 107/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5915 - val_loss: 0.6937\n",
            "Epoch 108/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5877 - val_loss: 0.6930\n",
            "Epoch 109/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5892 - val_loss: 0.6924\n",
            "Epoch 110/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5880 - val_loss: 0.6917\n",
            "Epoch 111/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5874 - val_loss: 0.6911\n",
            "Epoch 112/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5829 - val_loss: 0.6904\n",
            "Epoch 113/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5829 - val_loss: 0.6898\n",
            "Epoch 114/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5837 - val_loss: 0.6892\n",
            "Epoch 115/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5812 - val_loss: 0.6886\n",
            "Epoch 116/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5812 - val_loss: 0.6880\n",
            "Epoch 117/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5820 - val_loss: 0.6873\n",
            "Epoch 118/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5774 - val_loss: 0.6868\n",
            "Epoch 119/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5792 - val_loss: 0.6862\n",
            "Epoch 120/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5761 - val_loss: 0.6855\n",
            "Epoch 121/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5749 - val_loss: 0.6850\n",
            "Epoch 122/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5771 - val_loss: 0.6844\n",
            "Epoch 123/700\n",
            "621/621 [==============================] - 0s 24us/step - loss: 0.5759 - val_loss: 0.6838\n",
            "Epoch 124/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5739 - val_loss: 0.6832\n",
            "Epoch 125/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5724 - val_loss: 0.6826\n",
            "Epoch 126/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5728 - val_loss: 0.6821\n",
            "Epoch 127/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5731 - val_loss: 0.6815\n",
            "Epoch 128/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5698 - val_loss: 0.6810\n",
            "Epoch 129/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5681 - val_loss: 0.6804\n",
            "Epoch 130/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5677 - val_loss: 0.6799\n",
            "Epoch 131/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5689 - val_loss: 0.6794\n",
            "Epoch 132/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5686 - val_loss: 0.6787\n",
            "Epoch 133/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5653 - val_loss: 0.6782\n",
            "Epoch 134/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5644 - val_loss: 0.6777\n",
            "Epoch 135/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5665 - val_loss: 0.6771\n",
            "Epoch 136/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5648 - val_loss: 0.6765\n",
            "Epoch 137/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5654 - val_loss: 0.6761\n",
            "Epoch 138/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5651 - val_loss: 0.6756\n",
            "Epoch 139/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5605 - val_loss: 0.6751\n",
            "Epoch 140/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5631 - val_loss: 0.6745\n",
            "Epoch 141/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5611 - val_loss: 0.6740\n",
            "Epoch 142/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5577 - val_loss: 0.6734\n",
            "Epoch 143/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5605 - val_loss: 0.6729\n",
            "Epoch 144/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5548 - val_loss: 0.6724\n",
            "Epoch 145/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5581 - val_loss: 0.6719\n",
            "Epoch 146/700\n",
            "621/621 [==============================] - 0s 22us/step - loss: 0.5571 - val_loss: 0.6713\n",
            "Epoch 147/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5563 - val_loss: 0.6707\n",
            "Epoch 148/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5565 - val_loss: 0.6702\n",
            "Epoch 149/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5532 - val_loss: 0.6696\n",
            "Epoch 150/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5511 - val_loss: 0.6691\n",
            "Epoch 151/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5529 - val_loss: 0.6686\n",
            "Epoch 152/700\n",
            "621/621 [==============================] - 0s 26us/step - loss: 0.5533 - val_loss: 0.6681\n",
            "Epoch 153/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5505 - val_loss: 0.6675\n",
            "Epoch 154/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5517 - val_loss: 0.6670\n",
            "Epoch 155/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5534 - val_loss: 0.6665\n",
            "Epoch 156/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5519 - val_loss: 0.6660\n",
            "Epoch 157/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5487 - val_loss: 0.6655\n",
            "Epoch 158/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5481 - val_loss: 0.6650\n",
            "Epoch 159/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5468 - val_loss: 0.6644\n",
            "Epoch 160/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5473 - val_loss: 0.6640\n",
            "Epoch 161/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5461 - val_loss: 0.6635\n",
            "Epoch 162/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5439 - val_loss: 0.6629\n",
            "Epoch 163/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5452 - val_loss: 0.6624\n",
            "Epoch 164/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5453 - val_loss: 0.6619\n",
            "Epoch 165/700\n",
            "621/621 [==============================] - 0s 25us/step - loss: 0.5450 - val_loss: 0.6613\n",
            "Epoch 166/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5430 - val_loss: 0.6608\n",
            "Epoch 167/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5427 - val_loss: 0.6603\n",
            "Epoch 168/700\n",
            "621/621 [==============================] - 0s 27us/step - loss: 0.5442 - val_loss: 0.6598\n",
            "Epoch 169/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5402 - val_loss: 0.6593\n",
            "Epoch 170/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5440 - val_loss: 0.6587\n",
            "Epoch 171/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5433 - val_loss: 0.6583\n",
            "Epoch 172/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5362 - val_loss: 0.6578\n",
            "Epoch 173/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5410 - val_loss: 0.6573\n",
            "Epoch 174/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5396 - val_loss: 0.6567\n",
            "Epoch 175/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5360 - val_loss: 0.6562\n",
            "Epoch 176/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5365 - val_loss: 0.6557\n",
            "Epoch 177/700\n",
            "621/621 [==============================] - 0s 27us/step - loss: 0.5377 - val_loss: 0.6551\n",
            "Epoch 178/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5349 - val_loss: 0.6546\n",
            "Epoch 179/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5324 - val_loss: 0.6541\n",
            "Epoch 180/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5336 - val_loss: 0.6536\n",
            "Epoch 181/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5371 - val_loss: 0.6530\n",
            "Epoch 182/700\n",
            "621/621 [==============================] - 0s 24us/step - loss: 0.5362 - val_loss: 0.6525\n",
            "Epoch 183/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5317 - val_loss: 0.6520\n",
            "Epoch 184/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5322 - val_loss: 0.6515\n",
            "Epoch 185/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5291 - val_loss: 0.6510\n",
            "Epoch 186/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5347 - val_loss: 0.6505\n",
            "Epoch 187/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5319 - val_loss: 0.6499\n",
            "Epoch 188/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5319 - val_loss: 0.6495\n",
            "Epoch 189/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5307 - val_loss: 0.6489\n",
            "Epoch 190/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5305 - val_loss: 0.6484\n",
            "Epoch 191/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5301 - val_loss: 0.6479\n",
            "Epoch 192/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5285 - val_loss: 0.6474\n",
            "Epoch 193/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5288 - val_loss: 0.6469\n",
            "Epoch 194/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5268 - val_loss: 0.6463\n",
            "Epoch 195/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5312 - val_loss: 0.6458\n",
            "Epoch 196/700\n",
            "621/621 [==============================] - 0s 27us/step - loss: 0.5252 - val_loss: 0.6454\n",
            "Epoch 197/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5270 - val_loss: 0.6449\n",
            "Epoch 198/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5247 - val_loss: 0.6444\n",
            "Epoch 199/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5239 - val_loss: 0.6439\n",
            "Epoch 200/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5241 - val_loss: 0.6434\n",
            "Epoch 201/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5257 - val_loss: 0.6428\n",
            "Epoch 202/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5225 - val_loss: 0.6423\n",
            "Epoch 203/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5236 - val_loss: 0.6418\n",
            "Epoch 204/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5189 - val_loss: 0.6414\n",
            "Epoch 205/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5202 - val_loss: 0.6410\n",
            "Epoch 206/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5232 - val_loss: 0.6405\n",
            "Epoch 207/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5213 - val_loss: 0.6400\n",
            "Epoch 208/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5204 - val_loss: 0.6395\n",
            "Epoch 209/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5196 - val_loss: 0.6391\n",
            "Epoch 210/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5166 - val_loss: 0.6387\n",
            "Epoch 211/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5204 - val_loss: 0.6383\n",
            "Epoch 212/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5225 - val_loss: 0.6378\n",
            "Epoch 213/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5193 - val_loss: 0.6374\n",
            "Epoch 214/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5157 - val_loss: 0.6369\n",
            "Epoch 215/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5159 - val_loss: 0.6365\n",
            "Epoch 216/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5230 - val_loss: 0.6361\n",
            "Epoch 217/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5143 - val_loss: 0.6356\n",
            "Epoch 218/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5173 - val_loss: 0.6351\n",
            "Epoch 219/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5162 - val_loss: 0.6347\n",
            "Epoch 220/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5167 - val_loss: 0.6343\n",
            "Epoch 221/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5143 - val_loss: 0.6338\n",
            "Epoch 222/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5163 - val_loss: 0.6334\n",
            "Epoch 223/700\n",
            "621/621 [==============================] - 0s 23us/step - loss: 0.5110 - val_loss: 0.6331\n",
            "Epoch 224/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5134 - val_loss: 0.6327\n",
            "Epoch 225/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5103 - val_loss: 0.6323\n",
            "Epoch 226/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5139 - val_loss: 0.6319\n",
            "Epoch 227/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5144 - val_loss: 0.6315\n",
            "Epoch 228/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5150 - val_loss: 0.6311\n",
            "Epoch 229/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5101 - val_loss: 0.6308\n",
            "Epoch 230/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5093 - val_loss: 0.6303\n",
            "Epoch 231/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5139 - val_loss: 0.6300\n",
            "Epoch 232/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5078 - val_loss: 0.6296\n",
            "Epoch 233/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5146 - val_loss: 0.6293\n",
            "Epoch 234/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5129 - val_loss: 0.6289\n",
            "Epoch 235/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5103 - val_loss: 0.6286\n",
            "Epoch 236/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5120 - val_loss: 0.6283\n",
            "Epoch 237/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5087 - val_loss: 0.6280\n",
            "Epoch 238/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5087 - val_loss: 0.6276\n",
            "Epoch 239/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5086 - val_loss: 0.6272\n",
            "Epoch 240/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5083 - val_loss: 0.6270\n",
            "Epoch 241/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5062 - val_loss: 0.6266\n",
            "Epoch 242/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5104 - val_loss: 0.6263\n",
            "Epoch 243/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5137 - val_loss: 0.6260\n",
            "Epoch 244/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5059 - val_loss: 0.6257\n",
            "Epoch 245/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5061 - val_loss: 0.6254\n",
            "Epoch 246/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5140 - val_loss: 0.6250\n",
            "Epoch 247/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5041 - val_loss: 0.6248\n",
            "Epoch 248/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5067 - val_loss: 0.6246\n",
            "Epoch 249/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5045 - val_loss: 0.6243\n",
            "Epoch 250/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5058 - val_loss: 0.6240\n",
            "Epoch 251/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5034 - val_loss: 0.6237\n",
            "Epoch 252/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5034 - val_loss: 0.6235\n",
            "Epoch 253/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5040 - val_loss: 0.6233\n",
            "Epoch 254/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5033 - val_loss: 0.6231\n",
            "Epoch 255/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5073 - val_loss: 0.6229\n",
            "Epoch 256/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5055 - val_loss: 0.6227\n",
            "Epoch 257/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5025 - val_loss: 0.6225\n",
            "Epoch 258/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5060 - val_loss: 0.6223\n",
            "Epoch 259/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5052 - val_loss: 0.6221\n",
            "Epoch 260/700\n",
            "621/621 [==============================] - 0s 22us/step - loss: 0.4990 - val_loss: 0.6218\n",
            "Epoch 261/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5025 - val_loss: 0.6216\n",
            "Epoch 262/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5021 - val_loss: 0.6214\n",
            "Epoch 263/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5034 - val_loss: 0.6213\n",
            "Epoch 264/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5037 - val_loss: 0.6211\n",
            "Epoch 265/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5038 - val_loss: 0.6208\n",
            "Epoch 266/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5004 - val_loss: 0.6207\n",
            "Epoch 267/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5027 - val_loss: 0.6205\n",
            "Epoch 268/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5017 - val_loss: 0.6203\n",
            "Epoch 269/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5031 - val_loss: 0.6201\n",
            "Epoch 270/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.5018 - val_loss: 0.6199\n",
            "Epoch 271/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5026 - val_loss: 0.6198\n",
            "Epoch 272/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5028 - val_loss: 0.6197\n",
            "Epoch 273/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5017 - val_loss: 0.6195\n",
            "Epoch 274/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5000 - val_loss: 0.6194\n",
            "Epoch 275/700\n",
            "621/621 [==============================] - 0s 16us/step - loss: 0.4987 - val_loss: 0.6194\n",
            "Epoch 276/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5001 - val_loss: 0.6192\n",
            "Epoch 277/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5034 - val_loss: 0.6192\n",
            "Epoch 278/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5028 - val_loss: 0.6192\n",
            "Epoch 279/700\n",
            "621/621 [==============================] - 0s 21us/step - loss: 0.5010 - val_loss: 0.6191\n",
            "Epoch 280/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.5024 - val_loss: 0.6191\n",
            "Epoch 281/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5001 - val_loss: 0.6190\n",
            "Epoch 282/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.4988 - val_loss: 0.6188\n",
            "Epoch 283/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5016 - val_loss: 0.6188\n",
            "Epoch 284/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5011 - val_loss: 0.6186\n",
            "Epoch 285/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.4996 - val_loss: 0.6187\n",
            "Epoch 286/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.4976 - val_loss: 0.6185\n",
            "Epoch 287/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.5011 - val_loss: 0.6185\n",
            "Epoch 288/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5023 - val_loss: 0.6185\n",
            "Epoch 289/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.4993 - val_loss: 0.6185\n",
            "Epoch 290/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.4963 - val_loss: 0.6184\n",
            "Epoch 291/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.4952 - val_loss: 0.6184\n",
            "Epoch 292/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.4914 - val_loss: 0.6183\n",
            "Epoch 293/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.4976 - val_loss: 0.6181\n",
            "Epoch 294/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.4972 - val_loss: 0.6180\n",
            "Epoch 295/700\n",
            "621/621 [==============================] - 0s 19us/step - loss: 0.4952 - val_loss: 0.6180\n",
            "Epoch 296/700\n",
            "621/621 [==============================] - 0s 17us/step - loss: 0.4971 - val_loss: 0.6180\n",
            "Epoch 297/700\n",
            "621/621 [==============================] - 0s 20us/step - loss: 0.5005 - val_loss: 0.6179\n",
            "Epoch 298/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.5071 - val_loss: 0.6179\n",
            "Epoch 299/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.4957 - val_loss: 0.6179\n",
            "Epoch 300/700\n",
            "621/621 [==============================] - 0s 18us/step - loss: 0.4943 - val_loss: 0.6179\n",
            "Epoch 301/700\n",
            "621/621 [==============================] - 0s 27us/step - loss: 0.4991 - val_loss: 0.6179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYZUQdL-8lrS",
        "colab_type": "code",
        "outputId": "2281f2ea-ffee-45e1-bfe6-314c1f546d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Plot the training loss.\n",
        "plt.plot(history.history['loss'], 'r-')\n",
        "\n",
        "# Plot the validation loss.\n",
        "plt.plot(history.history['val_loss'], 'b-')\n",
        "\n",
        "# X-axis label.\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "# Y-axis label.\n",
        "plt.ylabel('Cost')\n",
        "\n",
        "# Graph legend.\n",
        "plt.legend([\"Training loss\", \"Validation loss\"])\n",
        "\n",
        "# Graph title.\n",
        "plt.title('Loss Graph')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}